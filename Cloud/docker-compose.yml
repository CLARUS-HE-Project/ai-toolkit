version: '3'


x-common:
  &common
  build:
      context: ./airflow
      dockerfile: Dockerfile
  image: apache/airflow:2.3.0
  user: "${AIRFLOW_UID}:0"
  env_file: 
    - .env
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
    - AWS_ACCESS_KEY_ID=${MINIO_USER}
    - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}
    - MLFLOW_S3_ENDPOINT_URL=${MINIO_ENDPOINT}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/src:/opt/airflow/src
    - /var/run/docker.sock:/var/run/docker.sock

x-depends-on:
  &depends-on
  depends_on:
    postgres:
        condition: service_healthy
    airflow-init:
        condition: service_completed_successfully

services:
    jupyter:
        build:
            context: ./jupyter
            dockerfile: Dockerfile
            args:
                JUPYTER_PASSWORD: ${JUPYTER_PASSWORD}
        image: dev/jupyter
        container_name: local_jupyter
        restart: always
        hostname: local_jupyter
        volumes:
            - ./jupyter/notebooks:/jupyter/notebooks
        ports:
            - ${JUPYTER_PORT}:8888
        environment:
            - SHM_SIZE=12gb
            - AWS_ACCESS_KEY_ID=${MINIO_USER}
            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}
            - MLFLOW_S3_ENDPOINT_URL=${MINIO_ENDPOINT}
        networks:
            - network_mlops

    postgres:
        image: postgres:12
        container_name: local_postgres
        restart: always
        environment:
            - POSTGRES_DATABASE=${POSTGRES_DATABASE}   # MLFLOW
            - POSTGRES_USER=${POSTGRES_USER}
            - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
            - POSTGRES_DATABASE_2=${POSTGRES_DATABASE_2}
            - POSTGRES_USER_2=${POSTGRES_USER_2}
            - POSTGRES_PASSWORD_2=${POSTGRES_PASSWORD_2}
            
        volumes:
            - pgdata:/var/lib/postgresql/data
            - ./postgres/init.sh:/docker-entrypoint-initdb.d/init.sh
        ports:
            - ${POSTGRES_PORT}:5432
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        networks:
            - network_mlops
        logging:
            options:
                max-size: 10m
                max-file: "3"

    minio:
        restart: always
        build:
            context: ./minio
            dockerfile: Dockerfile
        image: local_minio
        container_name: local_minio
        ports:
            - ${MINIO_PORT}:9000
            - ${MINIO_PORT_2}:9002
        networks:
            - network_mlops
        volumes:
            - miniodata:/data
        environment:
            - MINIO_ROOT_USER=${MINIO_USER}
            - MINIO_ROOT_PASSWORD=${MINIO_PASS}
        logging:
           options:
             max-size: 10m
             max-file: "3"
        command: server /data --console-address ":9002" --address ':9000'

    create_buckets:
        image: minio/mc:RELEASE.2019-07-17T22-13-42Z
        depends_on:
            - minio
        networks:
            - network_mlops
        entrypoint: >
            /bin/sh -c '
            sleep 5;
            /usr/bin/mc config host add s3 ${MINIO_ENDPOINT} ${MINIO_USER} ${MINIO_PASS} --api S3v4;
            [[ ! -z "`/usr/bin/mc ls s3 | grep challenge`" ]] || /usr/bin/mc mb s3/${MINIO_FIRST_BUCKET};
            /usr/bin/mc policy download s3/${MINIO_FIRST_BUCKET};
            exit 0;
            '

    mlflow:
        restart: always
        build:
            context: ./mlflow
            dockerfile: Dockerfile
        image: local_mlflow
        container_name: local_mlflow
        ports:
            - ${MLFLOW_PORT}:5000
        networks:
            - network_mlops
        environment:
            - AWS_ACCESS_KEY_ID=${MINIO_USER}
            - AWS_SECRET_ACCESS_KEY=${MINIO_PASS}
            - AWS_DEFAULT_REGION=us-east-1
            - MLFLOW_S3_ENDPOINT_URL=${MINIO_ENDPOINT}
            - MLFLOW_TRACKING_USERNAME=${MLFLOW_TRACKING_USERNAME}
            - MLFLOW_TRACKING_PASSWORD=${MLFLOW_TRACKING_PASSWORD}
            - MLFLOW_EXPERIMENT_DELETE_ENABLED=true
        depends_on:
            - postgres
            - minio
        command: mlflow server --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:${POSTGRES_PORT}/${POSTGRES_DATABASE} --default-artifact-root s3://${MINIO_FIRST_BUCKET} --host 0.0.0.0        

        
    scheduler:
        <<: [*common, *depends-on]
        container_name: airflow-scheduler
        command: scheduler
        networks:
            - network_mlops
        restart: on-failure
        ports:
        - ${AIRFLOW_SCHEDULER_PORT}:8793

    webserver:
        <<: [*common, *depends-on]
        container_name: airflow-webserver
        restart: always
        environment:
            - IP_CONNECTOR=${IP_CONNECTOR}
            - PORT_CONNECTOR=${PORT_CONNECTOR}
        command: webserver
        networks:
            - network_mlops
        ports:
        - ${AIRFLOW_WEBSERVER_PORT}:8080
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 30s
            timeout: 30s
            retries: 5
    
    airflow-init:
        <<: *common
        container_name: airflow-init
        entrypoint: /bin/bash
        networks:
            - network_mlops       
        command:
        - -c
        - |
            mkdir -p /sources/logs /sources/dags /sources/plugins /sources/src
            chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins,src}
            exec /entrypoint airflow version

networks:
    network_mlops:
        driver: bridge
        name: network_mlops

volumes: 
     pgdata:
     miniodata: